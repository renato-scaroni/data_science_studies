{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we prepare data for a bike share analytics platform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "6_min"
    ]
   },
   "source": [
    "## Exploring the data\n",
    "\n",
    "Dealing with unclean data is not necessarily glamorous, yet it is always essential. A common example would be a repetitive typo; e.g. 1% of the rows having \"New York\" spelled as \"New Yok\". We can use a combination of `.groupby()` and `.count()` in order to get a summary of the different values in a particular column, and notice that there are some common misspelled proper nouns.\n",
    "\n",
    "It can also be a good habit to shuffle the data and sift through a few hundred/thousand rows manually to get a good \"feel\" for the data. While this may seem excessive, you may catch an insight that would dramatically accelerate your subsequent work. Learn to love manually digging into the data if you want to be a data professional.\n",
    "\n",
    "Let's read the data using the `.read_csv()` method. Note that we are passing a parameter `nrows=10000` so that we only read in the first 10,000 rows in the file (we are working with a subset of the data on our local machine as the entire dataset is quite large - more than 3 million rows):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data file and take a look at the data\n",
    "df = pd.read_csv('data/trips.csv', nrows=10000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed before, the first step we ought to take with a new dataset is to familiarize ourselves with it. Let's read through the columns present in the dataset, find out how the data is spread out across columns, etc. This will also give us a sense of the obvious cleaning steps to be performed on each column present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of available features is as follows:\n",
    "\n",
    "1. **Rental Id:** A user can purchase the right to pick any bike in the city within 24 hours for £2. This column contains the ID of the rental (which may correspond to more than one bike). Notice that this is *not* the ID of the journey. Data type: `int64`.\n",
    "1. **Duration:** The duration of the journey in seconds. Data type: `int64`.\n",
    "2. **Bike Id:** The ID of the bike. Data type: `int64`.\n",
    "3. **End Date:** End time of the journey. Data type: `object`.\n",
    "4. **EndStation Id:** ID of the station at which this journey ended. Data type: `int64`.\n",
    "5. **Start Date:** Start time of the journey. Data type: `object`.\n",
    "6. **StartStation Id:** ID of the station at which this journey started. Data type: `int64`.\n",
    "7. **tag:** A tag that one of the members of your team assigned to each journey to make it easier to group them for further analysis. This column was not part of the original dataset. Data type: `object`.\n",
    "8. **userCategory:** Can be either `A` (occasional user) or `B` (frequent user). Data type: `object`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "1_min"
    ]
   },
   "source": [
    "## Format\n",
    "\n",
    "![Format icon](data/images/format.jpg)\n",
    "\n",
    "As mentioned during lecture, we will be handing our dataset as a `.csv` file encoded using `UTF-8`. We will make sure that these two requirements are met by exporting our file with the `df.to_csv(\"clean.csv\", encoding=\"utf-8\")` command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "1_min"
    ]
   },
   "source": [
    "## Relevance\n",
    "\n",
    "![Relevance icon](data/images/relevance.jpg)\n",
    "\n",
    "We said that we needed to:\n",
    "\n",
    "* Replace each missing `Bike Id` with the \"Not available\" string\n",
    "* Interpolate the missing values in the `userCategory` column\n",
    "* Replace each missing `EndStation Id` with the \"Not available\" string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "5_min"
    ]
   },
   "source": [
    "### Example 1\n",
    "\n",
    "Replace the missing values in `bike_id` and `end_station_id` with the \"Not Available\" string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** There are a number of ways to do this, but we will do so with the [**`.fillna()`**](https://pandas.pydata.org/docs/reference/api/pandas.Series.fillna.html) Series function. This function takes a single argument and replaces all `NaN` values in the Series with the value of that argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Bike Id\"] = df[\"Bike Id\"].fillna('Not available')\n",
    "df[\"EndStation Id\"] = df[\"EndStation Id\"].fillna('Not available')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "12_min"
    ]
   },
   "source": [
    "### Interpolating missing values\n",
    "\n",
    "To do the interpolation for `userCategory`, we can use the [**`.interpolate()`**](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.interpolate.html) method in `pandas`. The `interpolate()` method uses linear interpolation, which is a mathematical method for filling in unknown points based on building a linear regression model on the non-missing points (you will learn more about linear regression in future cases). We can then use this model to estimate the values of the missing points. This is *very* different from substituting null values with random or meaningless values, as it *preserves aspects of the distribution of the data, which can be very important for certain analyses*.\n",
    "\n",
    "But `userCategory` is a string - how can we apply a mathematical model to a string? Luckily, `userCategory` can only take on two values, so we can convert it to a `category` type with `.astype()` and then run the interpolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a category type\n",
    "df['userCategory'] = df['userCategory'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After converting `userCategory` to be a category column, let's see what the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The categories\n",
    "df['userCategory'].cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the codes assigned to the categories\n",
    "df['userCategory'].cat.codes.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see there are 2 codes here: 0 and 1, which correspond to the two values `['A']` and `['B']`. (The number -1 represents the NaN values.) In order to interpolate the values, we need to call the `.interpolate()` method after having replaced the -1s with actual null values (`np.nan`s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below code replaces the value -1 with NaN\n",
    "user_cat_codes = df['userCategory'].cat.codes.replace(-1, np.nan)\n",
    "\n",
    "# We now call the interpolate function that actually fills the NaN values with either a 0 or 1\n",
    "user_cat_codes = user_cat_codes.interpolate()\n",
    "user_cat_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert the codes to the `category` data type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_cat_codes = user_cat_codes.astype(int).astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we replace back the 0s and 1s with the actual category names. For this, we can use the **`.cat.rename_categories()`** method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_cat_codes = user_cat_codes.cat.rename_categories(df['userCategory'].cat.categories)\n",
    "df['userCategory'] = user_cat_codes\n",
    "df['userCategory']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "1_min"
    ]
   },
   "source": [
    "## Consistency\n",
    "\n",
    "![Consistency icon](data/images/consistency.jpg)\n",
    "\n",
    "### Data type consistency\n",
    "\n",
    "From our previous examination of the dataset, we decided that these changes had to be made:\n",
    "\n",
    "| Column | Current data type | Convert to |\n",
    "| --- | --- | --- |\n",
    "| `Rental Id` | `int64` | `category` |\n",
    "| `Bike Id` | `int64` | `category` |\n",
    "| `End Date` | `object` | `datetime` |\n",
    "| `EndStation Id` | `int64` | `category` |\n",
    "| `Start Date` | `object` | `datetime` |\n",
    "| `StartStation Id` | `int64` | `category` |\n",
    "| `tag` | `object` | `category` |\n",
    "| `userCategory` | `object` | `category` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "5_min"
    ]
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "To change the data type of a Series in `pandas`, we use the `.astype()` method that you learned in previous cases. Make the changes for all the columns except for `Start Date` and `End Date` (we will explain how to do those shortly). You also do not need to convert `userCategory` since we already did that when we interpolated its missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "3_min"
    ]
   },
   "source": [
    "### Converting dates\n",
    "\n",
    "This is a typical date in the dataset: `12/08/2020 16:28`. It follows the `DD/MM/YYYY` date format, and the 24 hour time format. The date and the time are separated by a space. To convert this kind of data into a `datetime` column, we use the **`pd.to_datetime()`** function like this (this function automatically detects the format, so we don't have to worry about that):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Start Date\"] = pd.to_datetime(df[\"Start Date\"])\n",
    "df[\"End Date\"] = pd.to_datetime(df[\"End Date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "5_min"
    ]
   },
   "source": [
    "### Exercise 2\n",
    "\n",
    "In terms of unit consistency, we need to create a new column, `duration_min`, that is the duration of the journeys in minutes instead of seconds. Create the column in `df`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now remove the `Duration` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"Duration\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "3_min"
    ]
   },
   "source": [
    "### Categorical consistency\n",
    "\n",
    "We noticed that in `tag` we are supposed to have only three categories, `low`, `medium`, and `high`, but according to our report, we have five. A closer look at our DataFrame reveals that we have these labels: \"priority low\", \"Priority low\", \"priority_high\", \"priority_medium\", and \"priority Medium\". We need to standardize these. We can make a replacement dictionary and rename our categories with this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict = {\"Priority low\":\"low\",\n",
    "               \"priority_high\":\"high\",\n",
    "               \"priority Medium\":\"medium\",\n",
    "               \"priority_medium\":\"medium\",\n",
    "               \"priority low\":\"low\"\n",
    "              }\n",
    "df[\"tag\"] = df[\"tag\"].replace(rename_dict).astype(\"category\")\n",
    "df[\"tag\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "5_min"
    ]
   },
   "source": [
    "### Exercise 3\n",
    "\n",
    "Do the appropriate replacements for the `userCategory` column (it has some weird formatting - `['A']` and `['B']` instead of `A` and `B`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "5_min"
    ]
   },
   "source": [
    "### Referential integrity\n",
    "\n",
    "We now need to remove the duplicates in the dataset. The method of choice is [**`.drop_duplicates()`**](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop_duplicates.html). When you use this method on a Series or DataFrame, it removes all the duplicate rows (keeping only the first occurrence) and then outputs the Series or DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default in `.drop_duplicates()` when used on a DataFrame is to treat as duplicates only the rows that are equal in all of their fields, but you can also ask `pandas` to check for duplicity only on a subset of the fields, using the `subset` argument.\n",
    "\n",
    "We also need to create trip IDs. When generating unique identifiers for datasets like this, we should make sure the generation process is **idempotent** (i.e. the same ID should be generated for each trip no matter how many times you run the script). This is required because there may be chances that the same trip is inputted into this tool multiple times. For example, suppose the customer first uploads the dataset for the first week of the month (for testing purposes, or based on data availability, etc.), and later uploads the dataset for the entire month. Now, if the same trip is assigned different IDs for each upload, then it might result in the analytics platform interpreting this as two different trips and this will skew the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "15_min"
    ]
   },
   "source": [
    "### Exercise 4\n",
    "\n",
    "Describe how you would generate a unique ID per trip while guaranteeing idempotence, then write code to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "3_min"
    ]
   },
   "source": [
    "### Column name consistency\n",
    "\n",
    "### Example 2\n",
    "\n",
    "Rename the columns so that they are in all-lowercase and separate words with underscores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** We can use the **`.rename()`** method to do this efficiently. Similar to `replace()`, this method can also take a dictionary as its argument to compress the amount of necessary code-writing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_dict = {\"Rental Id\":\"rental_id\",\n",
    "                \"Duration\":\"duration\",\n",
    "                \"Bike Id\":\"bike_id\",\n",
    "                \"End Date\":\"end_date\",\n",
    "                \"EndStation Id\":\"end_station_id\",\n",
    "                \"Start Date\":\"start_date\",\n",
    "                \"StartStation Id\":\"start_station_id\",\n",
    "                \"userCategory\":\"user_category\"\n",
    "               }\n",
    "df = df.rename(columns=columns_dict)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "2_min"
    ]
   },
   "source": [
    "## Data augmentation\n",
    "\n",
    "![Data augmentation](data/images/data_augmentation.jpg)\n",
    "\n",
    "Our priorities are:\n",
    "\n",
    "1. Merging `trips.csv` with `stations.json`\n",
    "2. Creating the following features:\n",
    "    1. Day of the week for both `start_date` and `end_date`\n",
    "    2. Hour for both `start_date` and `end_date`\n",
    "    3. Total cost of the rental\n",
    "    4. Location (the general area where the station is located). In the `stations.json` file, these are the strings after the comma in `Station name`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "25_min"
    ]
   },
   "source": [
    "### Merging the datasets\n",
    "\n",
    "To merge the two datasets, we first need to load `stations.json` into `pandas`. For this, we will use the [**`pd.read_json()`**](https://pandas.pydata.org/docs/reference/api/pandas.read_json.html) function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = pd.read_json(\"data/stations.json\", orient=\"columns\")\n",
    "stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**JSON files** are very similar to Python dictionaries. They are basically text files whose content is a dictionary that can be read by any major programming language.\n",
    "\n",
    "JSON files are sometimes used instead of CSV files to share data because in some cases they take up less memory. With `pd.read_json()`, you can read files like this and turn them into DataFrames with a single line of code. Depending on how the JSON file is built, you can specify the orientation with the `orient` argument. Above, we used `orient=\"columns\"` because the `stations.json` file's structure follows this pattern:\n",
    "\n",
    "~~~json\n",
    "{\n",
    "   \"col 1\":{\n",
    "      \"row 1\":\"This is column 1, row 1\",\n",
    "      \"row 2\":\"This is column 1, row 2\"\n",
    "   },\n",
    "   \"col 2\":{\n",
    "      \"row 1\":\"This is column 2, row 1\",\n",
    "      \"row 2\":\"This is column 2, row 2\"\n",
    "   }\n",
    "}\n",
    "~~~\n",
    "\n",
    "But let's say you have a file like this:\n",
    "\n",
    "~~~json\n",
    "{\n",
    "   \"row 1\":{\n",
    "      \"col 1\":\"This is row 1, column 1\",\n",
    "      \"col 2\":\"This is row 1, column 2\"\n",
    "   },\n",
    "   \"row 2\":{\n",
    "      \"col 1\":\"This is row 2, column 1\",\n",
    "      \"col 2\":\"This is row 2, column 2\"\n",
    "   }\n",
    "}\n",
    "~~~\n",
    "\n",
    "The correct way to read it would be using `orient=\"index\"` (since the default integer indexes in a DataFrame refer to rows, this makes logical sense)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = \"\"\"{\n",
    "   \"row 1\":{\n",
    "      \"col 1\":\"This is row 1, column 1\",\n",
    "      \"col 2\":\"This is row 1, column 2\"\n",
    "   },\n",
    "   \"row 2\":{\n",
    "      \"col 1\":\"This is row 2, column 1\",\n",
    "      \"col 2\":\"This is row 2, column 2\"\n",
    "   }\n",
    "}\"\"\"\n",
    "\n",
    "pd.read_json(j, orient=\"index\") # Try changing \"index\" for \"columns\" to see what happens!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create the `location` column here before doing the merge. For that, we use the [**`.str.split()`**](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.split.html) method. This method takes all the strings of a string Series and splits them using instances of a particular character within those strings. One common use case is when you have a column with strings that look like this:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">  <thead>    <tr style=\"text-align: right;\">      <th></th>      <th>my_series</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>Houston,Texas,United States</td>    </tr>    <tr>      <th>1</th>      <td>Toronto,Ontario,Canada</td>    </tr>    <tr>      <th>2</th>      <td>Tucson,Arizona,United States</td>    </tr>  </tbody></table>\n",
    "\n",
    "\n",
    "You can split these values using the comma as a separator with the code `my_series.str.split(pat=\",\")` to get this result:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">  <thead>    <tr style=\"text-align: right;\">      <th></th>      <th>my_series_split</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>[Houston, Texas, United States]</td>    </tr>    <tr>      <th>1</th>      <td>[Toronto, Ontario, Canada]</td>    </tr>    <tr>      <th>2</th>      <td>[Tucson, Arizona, United States]</td>    </tr>  </tbody></table>\n",
    "\n",
    "Notice how each string in this Series was turned into a list, where each element of this list was derived from the characters between consecutive instances of the \",\" character (or between the start of the string and the first \",\" for the first element, and the last \",\" and the end of the string for the last element).\n",
    "\n",
    "If you want the split values to be represented as columns of a DataFrame instead of lists inside a column, we can add the `expand=True` argument like this: `my_series.str.split(pat=\",\", expand=True)`. This is the result:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">  <thead>    <tr style=\"text-align: right;\">      <th></th>      <th>0</th>      <th>1</th>      <th>2</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>Houston</td>      <td>Texas</td>      <td>United States</td>    </tr>    <tr>      <th>1</th>      <td>Toronto</td>      <td>Ontario</td>      <td>Canada</td>    </tr>    <tr>      <th>2</th>      <td>Tucson</td>      <td>Arizona</td>      <td>United States</td>    </tr>  </tbody></table>\n",
    "\n",
    "Let's now apply this to our read-in JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_columns = stations[\"Station Name\"].str.split(pat=\",\", expand=True)\n",
    "split_columns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoops! Somehow a third column got appended and it seems to be `None` for all of the rows. How can this be? Well, if this is happening, it's likely that there is *some* station name which has TWO commas in it, and so it is getting split properly into three columns. But most other station names only have ONE comma in them, so splitting those into three columns would necessarily mean the last column would be empty. Let's see if we're right by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_columns[2].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems there are two stations with two commas in them. In order to deal with this, we use the `n` argument, which tells `pandas` the maximum number of splits `str.split()` should perform. For instance, you could use `my_string.str.split(pat=\",\", n=1, expand=True)` to split only at the first comma and get this result. Let's apply this to our read-in JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_columns = stations[\"Station Name\"].str.split(pat=\",\", n=1, expand=True)\n",
    "split_columns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go! Now we simply add these columns to `stations` and do the corresponding adjustments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = pd.concat([stations, split_columns], axis=1)\n",
    "rename_dict = {\n",
    "    0:\"station_name\",\n",
    "    1:\"location\",\n",
    "    \"Latitude\":\"latitude\",\n",
    "    \"Longitude\":\"longitude\",\n",
    "    \"Station ID\":\"station_id\"\n",
    "}\n",
    "stations = stations.rename(columns=rename_dict)\n",
    "stations = stations.drop(columns=[\"Station Name\"])\n",
    "stations[\"station_id\"] = stations[\"station_id\"].astype(\"category\") # We need this to be a category feature\n",
    "stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also use **`str.strip()`** to get rid of leading and trailing spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations[\"station_name\"] = stations[\"station_name\"].str.strip()\n",
    "stations[\"location\"] = stations[\"location\"].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`str.strip()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.strip.html) method takes all the strings in a Series and removes any leading or trailing whitespaces in them. After being processed with this method, a string like \"   hello world! \" will, for instance, be stripped to become \"hello world!\".\n",
    "\n",
    "You can also specify a custom character to remove with the `to_strip` argument (e.g. to remove `_`s you would use `my_series.str.strip(to_strip=\"_\")`, and \"\\_hello world!\\_\" would become \"hello world!\"), rather than removing whitespaces.\n",
    "\n",
    "To merge the datasets, we use `pd.merge()`. For `start_station_id`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, stations, left_on=\"start_station_id\", right_on=\"station_id\", how=\"left\")\n",
    "\n",
    "rename_dict = {\n",
    "    \"latitude\":\"start_latitude\",\n",
    "    \"longitude\":\"start_longitude\",\n",
    "    \"station_name\":\"start_station_name\",\n",
    "    \"location\":\"start_location\"\n",
    "}\n",
    "df = df.rename(columns=rename_dict)\n",
    "# Remove the extra column that was added\n",
    "df = df.drop(columns=[\"station_id\"])\n",
    "# Fill any remaining nulls with \"Not Available\" - only for string columns\n",
    "obj_cols = df.columns[df.dtypes==\"object\"]\n",
    "df[obj_cols] = df[obj_cols].fillna('Not Available')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "5_min"
    ]
   },
   "source": [
    "### Exercise 5\n",
    "\n",
    "Copy the code above and paste it in a new cell. Modify it to do the merge for the end stations this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "2_min"
    ]
   },
   "source": [
    "### Feature engineering\n",
    "\n",
    "Finally, let's create the `start_weekday`, `start_hour`, and `rental_cost` features. We start with the first two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've generated the merged df DataFrame for you, but you will have to figure out the code we used (in Exercise 5)!\n",
    "df = pd.read_csv(\"data/df_exercise.csv\", parse_dates=[\"start_date\"])\n",
    "\n",
    "df['start_hour'] = df['start_date'].dt.hour # dt.hour gets you the hour\n",
    "df['start_weekday'] = df['start_date'].dt.weekday # dt.weekday gets you the weekday (as an integer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we used the [**`dt.hour`**](https://pandas.pydata.org/docs/reference/api/pandas.Series.dt.hour.html) and [**`dt.weekday`**](https://pandas.pydata.org/docs/reference/api/pandas.Series.dt.weekday.html) attributes. These only work with columns that are of type `datetime`. These basically acess the hour and the weekday of the `datetime`. (Observe that you have to add `dt` before you call these attributes.) Other attributes include `dt.tz` (timezone), `dt.year`, `dt.month`, `dt.day`, `dt.minute`, and `dt.second`, [amongst others](https://pandas.pydata.org/docs/reference/series.html#datetimelike-properties)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "15_min"
    ]
   },
   "source": [
    "### Exercise 6 (optional)\n",
    "\n",
    "Create the `rental_cost` DataFrame. Remember that you have to pay £2 upfront for your rental, and then you can have as many rides as you want without paying more, so long as you don't exceed 30 minutes in each ride. If you exceed this threshold, you have to pay £2 for each 30-minute block (a fraction of a block counts as a full block).\n",
    "\n",
    "The DataFrame should look like this:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">  <thead>    <tr style=\"text-align: right;\">      <th></th>      <th>rental_id</th>      <th>rental_cost</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>101368080</td>      <td>2</td>    </tr>    <tr>      <th>1</th>      <td>101368081</td>      <td>2</td>    </tr>    <tr>      <th>2</th>      <td>101368082</td>      <td>2</td>    </tr>    <tr>      <th>3</th>      <td>101368083</td>      <td>2</td>    </tr>    <tr>      <th>4</th>      <td>101368084</td>      <td>2</td>    </tr>    <tr>      <th>5</th>      <td>101368086</td>      <td>2</td>    </tr>    <tr>      <th>6</th>      <td>101368172</td>      <td>2</td>    </tr>    <tr>      <th>7</th>      <td>101368173</td>      <td>2</td>    </tr>    <tr>      <th>8</th>      <td>101368174</td>      <td>2</td>    </tr>    <tr>      <th>9</th>      <td>101368176</td>      <td>2</td>    </tr>    <tr>      <th>...</th>      <td>...</td>      <td>...</td>    </tr>  </tbody></table>\n",
    "\n",
    "**Hint:** To round a float number up, you can use the [`.ceil()`](https://www.w3schools.com/python/ref_math_ceil.asp) function from the `math` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "3_min"
    ]
   },
   "source": [
    "### Joining the `rental_cost` data (optional)\n",
    "\n",
    "Now, we can merge the two DataFrames and make `trip_id` the index. We can do this using the `.set_index()` DataFrame method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've generated the rental_cost DataFrame for you, but you will have to figure out the code we used (in Exercise 6)!\n",
    "rental_cost = pd.read_csv(\"data/rental_cost.csv\")\n",
    "\n",
    "df = pd.merge(df, rental_cost, left_on=\"rental_id\", right_on=\"rental_id\", how=\"left\")\n",
    "df = df.set_index(\"trip_id\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "2_min"
    ]
   },
   "source": [
    "## Exporting the dataset\n",
    "\n",
    "Our dataset is now clean and ready to be exported and delivered to our client. We save it to our working directory using the [**`.to_csv()`**](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"clean.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method can take many arguments (check the [docs](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html) for details). Here we only used the desired filename (`\"clean.csv\"`) and `encoding=\"utf-8\"`, to make sure the result uses UTF-8 (this is the default value, so we could have also left this argument unspecified). If we wanted our file to be saved to a subfolder (say, `results/`), we could run `df.to_csv(\"results/clean.csv\", encoding=\"utf-8\")` instead."
   ]
  }
 ],
 "metadata": {
  "c1_recart": "6.7.0-57c20131aabc1dc2a8c675852d80a7da"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
